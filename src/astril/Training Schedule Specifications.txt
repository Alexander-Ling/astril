*Training Schedule Specifications*

Training schedules should be provided as a .tsv file where each row represents a set of parameters that are provided in columns. The available parameters are:

epoch: The first epoch in which these parameters should be applied.

scan_batch_size: *integer > 0* The number of scans to parse for training data per epoch. Limited by time and RAM. However, training data parsing automatically starts skipping/replacing slices if available RAM is exceeded before all scans have finished being parsed, so it is possible to select very large scan_batch_size values if you are not limited by time and want to shuffle partial coverage of all of the scans together.

slice_sub_batch_size: *integer > 0* The number of training slices to process together in a single sub batch. Especially important when training with a GPU, as larger sub batches can quickly exceed available GPU memory.

accumulate_n_sub_batches: *integer > 0* The number of sub-batches to average across before updating model weights. Larger values allows the model to “see” more scans before updating, especially if GPU memory requires small slice_sub_batch_size settings.

epochs_per_new_training_data: *integer > 0* The number of epochs to wait before loading new training data. Should usually be set to 1 (load new training data every epoch), but may be useful to set to values > 1 (re-use training data across multiple epochs) early in training to speed up model initialization.

conduct_validation: *TRUE or FALSE* Specifies whether model should be tested against validation set of scans.

validation_frequency: *integer > 0* If conduct_validation = TRUE, this determines how many epochs to wait between each validation cycle. Can be set to NA if conduct_validation = FALSE.

class_multiplication_factors: *dictionary-like string* Controls which (if any) slices should be duplicated (with random rotation) during training. Allows duplication of scans with important features that the model should see often. This is provided using a string similar to how dictionaries are defined in python. Interesting slices are identified by specifying a set of classes that the slice must contain all of, followed by an integer value for how many rotated duplicates of each identified slice should be added to the training set. For example a value of ‘{(3,4):10, (3,):2}’ would specify that slices that contain both classes 3 and 4 should be duplicated 10 times, while slices that contain class 3 should be duplicated two times. Note that slices are duplicated according to the *first* value in the dictionary that they match to. In this case, slices that contain only class 3 would be duplicated 2 times (because they don’t have class 4 and don’t match the first duplication criteria), and slices that contain classes 3 and 4 would be duplicated 10 times (because they match the first criteria) and would *not* be duplicated another 2 times, because the scan had already been matched by the (3,4) criteria. Scans that don't meet any of the provided criteria are included in the training dataset just one time (unrotated). *Note that criteria specifying a single class need to be followed by a trailing comma*. For example, {(3,):2} instead of {(3):2}. Can be set to NA.

require_classes: *dictionary-like string* Specifies inclusion/exclusion criteria for slices based on class presence. Defined like a python dictionary, with keys specifying a set of classes and values being "any", "all", or "none" to determine how to filter based on those classes. Slices are selected based on fitting *all* provided criteria. For example, a value of '{(1,2,3,4):"any",(5,):"none",(6,7):"all"}' would mean that, for a given epoch, the training set would only consider slices where scans: a) contain at least one of classes 1,2,3, or 4, b) do not contain class 5, and c) contain both classes 6 and 7. *Note that criteria specifying a single class need to be followed by a trailing comma*. For example, {(5,):"none"} instead of {(5):"none"}. Can be set to NA.

class_weights: *comma-separated number string* Specifies per-class training weights. Values should be > 0 and must include a weight for background (class = 0) as the first value. If not provided (i.e. blank or NA), the model will generate dynamic class weights based on class frequency. You can also specify some weights but not others. In this case, specified weights will be used for those classes while other classes will use dynamic weights. For example, if you specify '0.5,3,NA,NA,NA', classes 0 and 1 will have weights of 0.5 and 3 respectively, while classes 2, 3, and 4 will have dynamically calculcated class weights based on their frequency in the epoch training data.

tversky_alpha_values: *comma-separated number string* Specifies per-class alpha values to use in the Tversky loss function, with the first value corresponding to class 0, etc. Values should be between 0 and 1, with values = 0.5 providing even training against false positives and false negatives, values < 0.5 prioritizing true positives (the elimination of false negatives) for the given class, and values > 0.5 prioritizing true negatives (elimination of false positives) for the given class. For example '0.5,0.2,0.7' would correspond to alpha values of 0.5, 0.2, and 0.7 for classes 0, 1, and 2 respectively (class 0 = background).

tversky_gamma: *float > 0* Specified single numeric value > 0 to use as gamma in the Focal Tversky loss function. If gamma > 1, the Focal Tversky loss focuses more on hard examples. If gamma = 1, it's the standard Tversky-based loss (no focal effect). If gamma < 1, it downplays harder examples (not commonly used).

wce_loss_weight: *float >= 0 and <= 1* Single numeric value between 0 and 1 (inclusive) specifying the proprotion of loss that should be calculated using the Weighted Cross Entropy (WCE) loss function as opposed to Focal Tversky (FT) Loss. Loss is calcualted by combining both loss functions in a ratio defined as [Loss = WCE Loss * wce_loss_weight + FT Loss * (1 - wce_Loss_weight)]. Higher wce_loss_weight values favor training the model with global probabilistic calibration, while lower values favor training the model to ensure segmentation shape overlap alignment.

learning_rate: *float* Specifies the learning rate of the model during these epochs. If you don't know what this is, a general rule of thumb is that 0.001 is a relatively normal learning rate, and 0.0001 is about as slow as you'll need to go. You should generally avoid changing the learning rate too quickly. For example, you may want to "warm up" your model with a learning rate of 0.0001, then gradually increase to 0.001 for the bulk of the training, then gradually decrease again to 0.0001 to fine tune parameters near the end of training. Mileage will vary.
